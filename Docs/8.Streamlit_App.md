# Comprehensive Guide: Replicating the House Prices Streamlit App

## 1. Project Structure

Your project is organized as follows:
- `main.py`: Entry point for the Streamlit app, manages navigation and tab selection.
- `configs/config.py`: Stores tab names for navigation.
- `models/`: Contains pre-trained models (e.g., `linear_regression_model.pkl`).
  - `schemas.py`: Defines Pydantic schemas for input validation.
- `views/`: Contains three main Streamlit modules:
  - `house_price.py`: Predicts house prices using a pre-trained model.
  - `custom_linear_app.py`: Lets users train and use a custom linear regression model.
  - `custom_xgboost.py`: Lets users train and use a custom XGBoost model.

---

## 2. Replication Steps

### Step 1: Environment Setup

- Install Python (recommended: 3.11+).
- Install dependencies:
  ```bash
  pip install -r requirements.txt
  ```
  Or use `environment.yml` for conda:
  ```bash
  conda env create -f environment.yml
  conda activate <your_env_name>
  ```

### Step 2: Prepare Data & Models

- Place your data files in the `data/raw/` directory.
- Train the initial model using `data_cleaning_and_feature_engineering.ipynb` and save it as `models/linear_regression_model.pkl`.

### Step 3: Code Overview
#### `main.py`

`main.py` is the entry point for your Streamlit app. It sets up the page, handles sidebar navigation, and loads the appropriate module based on user selection.

Below is a simplified version of `main.py` with explanations for each part:

```python
import streamlit as st
from views.house_price import run_house_price_app
from views.custom_linear_app import custom_model_linear_regression_app
from views.custom_xgboost import custom_model_xgboost_app
from configs.config import HOUSE_PRICE_TAB, CUSTOM_APP_TAB, XGBOOST_APP_TAB

# Configure Streamlit page settings
st.set_page_config(page_title="House Prices App", layout="wide")

# Main page title
st.title("Linear Regression & XGBoost Module")

# Tab names for sidebar navigation
tab_names = [HOUSE_PRICE_TAB, CUSTOM_APP_TAB, XGBOOST_APP_TAB, "About"]

# Sidebar for navigation between tabs
with st.sidebar:
    st.header("Navigation")
    selected_tab = st.radio("Select a tab:", tab_names, index=0)

# Display content based on selected tab
if selected_tab == tab_names[0]:
    st.session_state["selected_tab"] = HOUSE_PRICE_TAB
    run_house_price_app()  # House price prediction module
elif selected_tab == tab_names[1]:
    st.session_state["selected_tab"] = CUSTOM_APP_TAB
    custom_model_linear_regression_app()  # Custom linear regression module
elif selected_tab == tab_names[2]:
    st.session_state["selected_tab"] = XGBOOST_APP_TAB
    custom_model_xgboost_app()  # Custom XGBoost module
elif selected_tab == tab_names[3]:
    st.session_state["selected_tab"] = "About"
    st.header("About")
    st.write("This app demonstrates house price prediction using Streamlit.")
```

**Explanation:**

- **Page Configuration:**  
  ```python
  st.set_page_config(page_title="House Prices App", layout="wide")
  ```
  Sets the page title and layout for optimal visualization.

- **Title and Navigation:**  
  ```python
  st.title("Linear Regression & XGBoost Module")
  with st.sidebar:
      st.header("Navigation")
      selected_tab = st.radio("Select a tab:", tab_names, index=0)
  ```
  Displays the main title and implements sidebar navigation for tab selection.

- **Tab Management:**  
  ```python
  tab_names = [HOUSE_PRICE_TAB, CUSTOM_APP_TAB, XGBOOST_APP_TAB, "About"]
  selected_tab = st.radio("Select a tab:", tab_names, index=0)
  st.session_state["selected_tab"] = selected_tab
  ```
  Imports tab names from `configs/config.py` and tracks the selected tab in session state.

- **Dynamic Content Loading:**  
  ```python
  if selected_tab == tab_names[0]:
      run_house_price_app()
  elif selected_tab == tab_names[1]:
      custom_model_linear_regression_app()
  elif selected_tab == tab_names[2]:
      custom_model_xgboost_app()
  elif selected_tab == tab_names[3]:
      st.header("About")
      st.write("This app demonstrates house price prediction using Streamlit.")
  ```
  Loads the appropriate module or displays information based on the selected tab.

- **Modular Design:**  
  Each tab's logic is separated into its own file under `views/`, making the codebase maintainable and extensible.

- **Session State Usage:**  
  ```python
  st.session_state["selected_tab"] = selected_tab
  ```
  Ensures tab selection and prediction history persist across user actions.

This structure provides a clean, user-friendly interface and makes it easy to add new features or tabs in the future.


#### `configs/config.py`

This file defines the tab names as constants, making navigation easy to manage and update.

```python
# configs/config.py

# Tab name for the custom linear regression application
CUSTOM_APP_TAB = "Custom Linear Regression"

# Tab name for the house price prediction application
HOUSE_PRICE_TAB = "House Price Prediction"


XGBOOST_APP_TAB = "Custom XGBoost"
```

**Explanation:**

- **Tab Name Constants:**  
  ```python
  # Tab name for the custom linear regression application
  CUSTOM_APP_TAB = "Custom Linear Regression"

  # Tab name for the house price prediction application
  HOUSE_PRICE_TAB = "House Price Prediction"

  XGBOOST_APP_TAB = "Custom XGBoost"
  ```
  - Each tab name is defined as a constant, making navigation easy to manage and update.
  - This approach improves maintainability and consistency across your codebase.


### `models/schemas.py`

This file defines the input schema for house price prediction using Pydantic, ensuring robust validation for each feature.

#### Full Code

```python
from pydantic import BaseModel, conint, confloat

# List of features used for prediction, with their types and constraints.
FEATURES = [
    ("AllSF", confloat(ge=0)),                # Total square footage, must be >= 0
    ("OverallQual", conint(ge=1, le=10)),     # Overall material and finish quality, 1-10
    ("NeighborPrice", confloat(ge=0)),        # Average price in neighborhood, must be >= 0
    ("GrLivArea", conint(ge=0)),              # Above ground living area, must be >= 0
    ("ExterQual_", conint(ge=0)),             # Exterior quality (encoded), must be >= 0
    ("NeighborBin", conint(ge=0)),            # Neighborhood bin (encoded), must be >= 0
    ("KitchenQual_", conint(ge=0)),           # Kitchen quality (encoded), must be >= 0
    ("SimplOverallQual", conint(ge=0)),       # Simplified overall quality (encoded), must be >= 0
    ("TotalBsmtSF", confloat(ge=0)),          # Total basement area, must be >= 0
    ("GarageCars", confloat(ge=0)),           # Number of garage cars, must be >= 0
    ("TotalBath", confloat(ge=0)),            # Total number of bathrooms, must be >= 0
    ("GarageScore", confloat(ge=0)),          # Garage score (custom feature), must be >= 0
    ("GarageArea", confloat(ge=0)),           # Garage area, must be >= 0
    ("1stFlrSF", conint(ge=0)),               # First floor square footage, must be >= 0
    ("BsmtQual_", conint(ge=0)),              # Basement quality (encoded), must be >= 0
    ("OverallGrade", conint(ge=0)),           # Overall grade (custom feature), must be >= 0
    ("FullBath", conint(ge=0)),               # Number of full bathrooms, must be >= 0
    ("TotRmsAbvGrd", conint(ge=0)),           # Total rooms above ground, must be >= 0
    ("Now_YearBuilt", conint(ge=0)),          # Years since built (custom feature), must be >= 0
    ("YearBuilt", conint(ge=1800, le=2024)),  # Year built, between 1800 and 2024
]

class GaragePredictRequest(BaseModel):
    AllSF: confloat(ge=0)
    OverallQual: conint(ge=1, le=10)
    NeighborPrice: confloat(ge=0)
    GrLivArea: conint(ge=0)
    ExterQual_: conint(ge=0)
    NeighborBin: conint(ge=0)
    KitchenQual_: conint(ge=0)
    SimplOverallQual: conint(ge=0)
    TotalBsmtSF: confloat(ge=0)
    GarageCars: confloat(ge=0)
    TotalBath: confloat(ge=0)
    GarageScore: confloat(ge=0)
    GarageArea: confloat(ge=0)
    # Typo in original: should be '1stFlrSF'
    stFlrSF: conint(ge=0)
    BsmtQual_: conint(ge=0)
    OverallGrade: conint(ge=0)
    FullBath: conint(ge=0)
    TotRmsAbvGrd: conint(ge=0)
    Now_YearBuilt: conint(ge=0)
    YearBuilt: conint(ge=1800, le=2024)
```

#### Key Parts & Explanation

**1. Feature List**

```python
FEATURES = [
    ("AllSF", confloat(ge=0)),
    ("OverallQual", conint(ge=1, le=10)),
    # ... more features ...
]
```
- Centralizes all features and their constraints for easy reference and dynamic UI generation.

**2. Pydantic Model**

```python
class GaragePredictRequest(BaseModel):
    AllSF: confloat(ge=0)
    OverallQual: conint(ge=1, le=10)
    # ... more fields ...
```
- Defines strict validation for each input, ensuring correct types and value ranges.
- Used to validate user input before making predictions.

**3. Constraints**

- `conint(ge=1, le=10)`: Integer between 1 and 10.
- `confloat(ge=0)`: Float greater than or equal to 0.

**4. Usage**

- The schema is used in the prediction view to dynamically generate input fields and validate requests, improving reliability and user experience.


#### `views/house_price.py`

This module powers the "House Price Prediction" tab. It loads a pre-trained model, dynamically generates input fields for features using Pydantic schemas, and displays predictions along with a chart of previous results.

Below is a simplified version of `views/house_price.py` with explanations for each part:

```python
import streamlit as st
from pydantic import conint, confloat
import pickle
import pandas as pd
from models.schemas import GaragePredictRequest, FEATURES
from configs.config import HOUSE_PRICE_TAB as TABE_NAME

def run_house_price_app():
    # Load the trained model from file
    try:
        model = pickle.load(open('models/linear_regression_model.pkl', 'rb'))
    except (FileNotFoundError, pickle.UnpicklingError) as e:
        st.error("Trained model file not found. Please train the model first by running the `data_cleaning_and_feature_engineering.ipynb` notebook.")
        return

    # Dynamically set attributes for the Pydantic request schema
    for fname, ftype in FEATURES:
        setattr(GaragePredictRequest, fname, ftype)

    # Streamlit UI: Title and input section
    st.title("House Price Prediction")
    st.subheader("Input Features")
    inputs = {}
    # Generate input fields based on feature types
    for fname, ftype in FEATURES:
        if ftype == confloat(ge=0):
            inputs[fname] = st.number_input(fname, min_value=0.0, value=0.0)
        elif ftype == conint(ge=1, le=10):
            inputs[fname] = st.number_input(fname, min_value=1, max_value=10, value=5)
        elif ftype == conint(ge=1800, le=2024):
            inputs[fname] = st.number_input(fname, min_value=1800, max_value=2024, value=2000)
        elif ftype == conint(ge=0):
            inputs[fname] = st.number_input(fname, min_value=0, value=0)
        else:
            inputs[fname] = st.number_input(fname, value=0)

    # Initialize session state for storing predictions
    if "predictions" not in st.session_state:
        st.session_state.predictions = []

    # Prediction logic triggered by button
    if st.button("Predict"):
        # Rename key for model compatibility
        inputs["stFlrSF"] = inputs.pop("1stFlrSF")  
        request = GaragePredictRequest(**inputs)

        # Prepare features for model input
        feature_dict = request.model_dump()
        feature_dict["1stFlrSF"] = feature_dict.pop("stFlrSF")
        features = pd.DataFrame([feature_dict])
        cols = features.columns.tolist()
        cols.remove("1stFlrSF")
        # Insert "1stFlrSF" at specific position for model
        insert_idx = -6 if len(cols) >= 5 else len(cols)
        cols.insert(insert_idx, "1stFlrSF")
        features = features[cols]
        
        # Make prediction and update session state
        prediction = model.predict(features)
        st.session_state.predictions.append(float(prediction[0]))
        st.success(f"Predicted Value: {float(prediction[0]):.2f}")

    # Show sidebar only if this tab is selected
    if st.session_state.get("selected_tab") == TABE_NAME:
        with st.sidebar:
            # Set sidebar width using custom CSS
            st.markdown(
                """
                <style>
                [data-testid="stSidebar"] {
                    min-width: 400px;
                    max-width: 400px;
                    width: 400px;
                }
                </style>
                """,
                unsafe_allow_html=True,
            )
            # Display line chart of predictions
            st.line_chart(st.session_state.predictions or [0,1,2,3,4])
```

**Explanation:**
**Code Snippets & Explanation:**

- **Model Loading:**
  ```python
  try:
      model = pickle.load(open('models/linear_regression_model.pkl', 'rb'))
  except (FileNotFoundError, pickle.UnpicklingError) as e:
      st.error("Trained model file not found. Please train the model first by running the `data_cleaning_and_feature_engineering.ipynb` notebook.")
      return
  ```
  Loads the pre-trained model. If missing, shows an error.

- **Dynamic Feature Schema Setup:**
  ```python
  for fname, ftype in FEATURES:
      setattr(GaragePredictRequest, fname, ftype)
  ```
  Dynamically sets feature types for Pydantic validation.

- **Streamlit UI for Inputs:**
  ```python
  st.title("House Price Prediction")
  st.subheader("Input Features")
  inputs = {}
  for fname, ftype in FEATURES:
      if ftype == confloat(ge=0):
          inputs[fname] = st.number_input(fname, min_value=0.0, value=0.0)
      elif ftype == conint(ge=1, le=10):
          inputs[fname] = st.number_input(fname, min_value=1, max_value=10, value=5)
      elif ftype == conint(ge=1800, le=2024):
          inputs[fname] = st.number_input(fname, min_value=1800, max_value=2024, value=2000)
      elif ftype == conint(ge=0):
          inputs[fname] = st.number_input(fname, min_value=0, value=0)
      else:
          inputs[fname] = st.number_input(fname, value=0)
  ```
  Generates input fields for each feature based on its type.

- **Prediction Button & Logic:**
  ```python
  if st.button("Predict"):
      inputs["stFlrSF"] = inputs.pop("1stFlrSF")
      request = GaragePredictRequest(**inputs)
      feature_dict = request.model_dump()
      feature_dict["1stFlrSF"] = feature_dict.pop("stFlrSF")
      features = pd.DataFrame([feature_dict])
      cols = features.columns.tolist()
      cols.remove("1stFlrSF")
      insert_idx = -6 if len(cols) >= 5 else len(cols)
      cols.insert(insert_idx, "1stFlrSF")
      features = features[cols]
      prediction = model.predict(features)
      st.session_state.predictions.append(float(prediction[0]))
      st.success(f"Predicted Value: {float(prediction[0]):.2f}")
  ```
  Handles prediction, input renaming, and result display.

- **Sidebar Chart:**
  ```python
  if st.session_state.get("selected_tab") == TABE_NAME:
      with st.sidebar:
          st.markdown(
              """
              <style>
              [data-testid="stSidebar"] {
                  min-width: 400px;
                  max-width: 400px;
                  width: 400px;
              }
              </style>
              """,
              unsafe_allow_html=True,
          )
          st.line_chart(st.session_state.predictions or [0,1,2,3,4])
  ```
  Shows a chart of previous predictions in the sidebar.

These snippets illustrate the main logic for loading the model, validating inputs, making predictions, and displaying results in the Streamlit app.

#### `views/custom_linear_app.py`

This module powers the "Custom Linear Regression" tab. It lets users upload their own CSV data, select features and target, train a linear regression model, view data summaries and metrics, and make predictions.

Below is a simplified version of `views/custom_linear_app.py` with explanations for each part:

```python
import streamlit as st
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns
from configs.config import CUSTOM_APP_TAB

def custom_model_linear_regression_app():
    # Set the title and tab for the Streamlit app
    st.title("Custom Model Linear Regression Training")
    st.session_state["selected_tab"] = CUSTOM_APP_TAB

    # File uploader for CSV data
    uploaded_file = st.file_uploader("Upload CSV file", type=["csv"])

    if uploaded_file:
        # Read uploaded CSV into DataFrame
        df = pd.read_csv(uploaded_file)
        st.write("Data Preview:", df.head())

        # Display summary statistics and missing values
        st.subheader("Data Summary")
        st.write("Number of missing values:")
        st.write(df.isnull().sum())
        st.write("Median values:")
        st.write(df.select_dtypes(include=['number']).median())
        st.write("Mean values:")
        st.write(df.select_dtypes(include=['number']).mean())

        # Calculate and display correlation matrix
        st.subheader("Correlation Matrix")
        corr_matrix = df.corr(numeric_only=True)
        st.write(corr_matrix)
        fig = plt.figure(figsize=(20, 16))
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", square=True, cbar_kws={"shrink": .8})
        st.pyplot(fig)

        # Select target and feature columns
        target = st.selectbox("Select label (Y) column", df.columns)
        features = st.multiselect(
            "Select feature columns (X)", [col for col in df.columns if col != target]
        )

        if features and target:
            # Initialize session state for model and feature info
            if "trained_model" not in st.session_state:
                st.session_state["trained_model"] = None
                st.session_state["numeric_features"] = []
                st.session_state["categorical_features"] = []

            # Button to start training
            if st.button("Start Training", key="start_training_btn"):
                # Drop rows with missing values in selected columns
                clean_df = df[features + [target]].dropna()
                X = clean_df[features]
                y = clean_df[target]

                # Identify numeric and categorical features
                numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
                categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()

                # Preprocessing pipeline for numeric and categorical features
                preprocessor = ColumnTransformer(
                    transformers=[
                        ('num', StandardScaler(), numeric_features),
                        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
                    ]
                )

                # Full pipeline: preprocessing + regression model
                model = Pipeline(steps=[
                    ('preprocessor', preprocessor),
                    ('regressor', LinearRegression())
                ])

                # Split data into train and test sets
                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

                # Train the model
                model.fit(X_train, y_train)
                score = model.score(X_test, y_test)

                # Make predictions on test set
                y_pred = model.predict(X_test)

                # Calculate evaluation metrics
                mae = mean_absolute_error(y_test, y_pred)
                mse = mean_squared_error(y_test, y_pred)

                # Display model performance
                st.write(f"Model R^2 score on test set: {score:.4f}")
                st.write(f"Mean Absolute Error (MAE): {mae:.4f}")
                st.write(f"Mean Squared Error (MSE): {mse:.4f}")

                # Save model and feature info in session state
                st.session_state["trained_model"] = model
                st.session_state["numeric_features"] = numeric_features
                st.session_state["categorical_features"] = categorical_features
                st.session_state["selected_features"] = features
                st.session_state["df"] = df

            # Prediction section: only show if model is trained
            if st.session_state.get("trained_model") is not None:
                st.subheader("Make a Prediction")
                input_data = {}
                df_for_input = st.session_state["df"]
                numeric_features = st.session_state["numeric_features"]
                categorical_features = st.session_state["categorical_features"]
                selected_features = st.session_state["selected_features"]

                # Input widgets for each feature
                for feature in selected_features:
                    if feature in numeric_features:
                        val = st.number_input(f"Input value for {feature}", value=float(df_for_input[feature].mean()))
                    else:
                        val = st.selectbox(f"Input value for {feature}", options=df_for_input[feature].unique())
                    input_data[feature] = val

                # Initialize prediction result in session state
                if "prediction_result" not in st.session_state:
                    st.session_state["prediction_result"] = None

                # Button to make prediction
                if st.button("Predict", key="predict_btn"):
                    input_df = pd.DataFrame([input_data])
                    pred = st.session_state["trained_model"].predict(input_df)[0]
                    st.session_state["prediction_result"] = f"Predicted value: {pred:.4f}"

                # Display prediction result
                if st.session_state.get("prediction_result"):
                    st.write(st.session_state["prediction_result"])
```

**Explanation:**

**Code Snippets & Explanation:**

- **CSV Upload & Data Preview:**
  ```python
  uploaded_file = st.file_uploader("Upload CSV file", type=["csv"])
  if uploaded_file:
      df = pd.read_csv(uploaded_file)
      st.write("Data Preview:", df.head())
  ```
  Lets users upload a CSV and shows the first few rows.

- **Data Summary & Correlation Matrix:**
  ```python
  st.subheader("Data Summary")
  st.write("Number of missing values:")
  st.write(df.isnull().sum())
  st.write("Median values:")
  st.write(df.select_dtypes(include=['number']).median())
  st.write("Mean values:")
  st.write(df.select_dtypes(include=['number']).mean())

  st.subheader("Correlation Matrix")
  corr_matrix = df.corr(numeric_only=True)
  st.write(corr_matrix)
  fig = plt.figure(figsize=(20, 16))
  sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", square=True, cbar_kws={"shrink": .8})
  st.pyplot(fig)
  ```
  Shows missing values, medians, means, and a heatmap of correlations.

- **Feature & Target Selection:**
  ```python
  target = st.selectbox("Select label (Y) column", df.columns)
  features = st.multiselect("Select feature columns (X)", [col for col in df.columns if col != target])
  ```
  Lets users pick the target and features for regression.

- **Model Training Pipeline:**
  ```python
  if st.button("Start Training", key="start_training_btn"):
      clean_df = df[features + [target]].dropna()
      X = clean_df[features]
      y = clean_df[target]

      numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
      categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()

      preprocessor = ColumnTransformer(
          transformers=[
              ('num', StandardScaler(), numeric_features),
              ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
          ]
      )

      model = Pipeline(steps=[
          ('preprocessor', preprocessor),
          ('regressor', LinearRegression())
      ])

      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
      model.fit(X_train, y_train)
      score = model.score(X_test, y_test)
      y_pred = model.predict(X_test)
      mae = mean_absolute_error(y_test, y_pred)
      mse = mean_squared_error(y_test, y_pred)

      st.write(f"Model R^2 score on test set: {score:.4f}")
      st.write(f"Mean Absolute Error (MAE): {mae:.4f}")
      st.write(f"Mean Squared Error (MSE): {mse:.4f}")
  ```
  Trains a linear regression model with preprocessing and displays metrics.

- **Prediction Input & Output:**
  ```python
  if st.session_state.get("trained_model") is not None:
      st.subheader("Make a Prediction")
      input_data = {}
      for feature in selected_features:
          if feature in numeric_features:
              val = st.number_input(f"Input value for {feature}", value=float(df_for_input[feature].mean()))
          else:
              val = st.selectbox(f"Input value for {feature}", options=df_for_input[feature].unique())
          input_data[feature] = val

      if st.button("Predict", key="predict_btn"):
          input_df = pd.DataFrame([input_data])
          pred = st.session_state["trained_model"].predict(input_df)[0]
          st.session_state["prediction_result"] = f"Predicted value: {pred:.4f}"

      if st.session_state.get("prediction_result"):
          st.write(st.session_state["prediction_result"])
  ```
  Lets users input feature values and shows the predicted result.

These snippets illustrate the main logic for uploading data, training a custom linear regression model, and making predictions interactively in Streamlit.

#### `views/custom_xgboost.py`

This module powers the "Custom XGBoost" tab. It allows users to upload their own CSV data, select features and target, train an XGBoost regression model, view data summaries and metrics, and make predictions.

Below is a simplified version of `views/custom_xgboost.py` with explanations for each part:

```python
import streamlit as st
import pandas as pd
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns
from configs.config import CUSTOM_APP_TAB

def custom_model_xgboost_app():
    # Set the title and tab for the Streamlit app
    st.title("Custom Model XGBoost Training")
    st.session_state["selected_tab"] = CUSTOM_APP_TAB

    # File uploader for CSV data
    uploaded_file = st.file_uploader("Upload CSV file", type=["csv"])

    if uploaded_file:
        # Read uploaded CSV into DataFrame
        df = pd.read_csv(uploaded_file)
        st.write("Data Preview:", df.head())

        # Show summary statistics and missing values
        st.subheader("Data Summary")
        st.write("Number of missing values:")
        st.write(df.isnull().sum())
        st.write("Median values:")
        st.write(df.select_dtypes(include=['number']).median())
        st.write("Mean values:")
        st.write(df.select_dtypes(include=['number']).mean())

        # Show correlation matrix and heatmap
        st.subheader("Correlation Matrix")
        corr_matrix = df.corr(numeric_only=True)
        st.write(corr_matrix)
        fig = plt.figure(figsize=(20, 16))
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", square=True, cbar_kws={"shrink": .8})
        st.pyplot(fig)

        # Select target and feature columns
        target = st.selectbox("Select label (Y) column", df.columns)
        features = st.multiselect(
            "Select feature columns (X)", [col for col in df.columns if col != target]
        )

        if features and target:
            # Initialize session state for model and feature info
            if "trained_model" not in st.session_state:
                st.session_state["trained_model"] = None
                st.session_state["numeric_features"] = []
                st.session_state["categorical_features"] = []

            # Button to start training
            if st.button("Start Training", key="start_training_btn"):
                # Drop rows with missing values in selected columns
                clean_df = df[features + [target]].dropna()
                X = clean_df[features]
                y = clean_df[target]

                # Identify numeric and categorical features
                numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
                categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()

                # Build preprocessing pipeline for numeric and categorical features
                preprocessor = ColumnTransformer(
                    transformers=[
                        ('num', StandardScaler(), numeric_features),
                        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
                    ]
                )

                # Build full pipeline: preprocessing + regression model
                model = Pipeline(steps=[
                    ('preprocessor', preprocessor),
                    ('regressor', XGBRegressor(max_depth=3, n_estimators=100, random_state=42))
                ])

                # Split data into train and test sets
                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

                # Train the model
                model.fit(X_train, y_train)
                score = model.score(X_test, y_test)

                # Make predictions on test set
                y_pred = model.predict(X_test)

                # Calculate evaluation metrics
                mae = mean_absolute_error(y_test, y_pred)
                mse = mean_squared_error(y_test, y_pred)

                # Show model performance
                st.write(f"Model R^2 score on test set: {score:.4f}")
                st.write(f"Mean Absolute Error (MAE): {mae:.4f}")
                st.write(f"Mean Squared Error (MSE): {mse:.4f}")

                # Save model and feature info in session state
                st.session_state["trained_model"] = model
                st.session_state["numeric_features"] = numeric_features
                st.session_state["categorical_features"] = categorical_features
                st.session_state["selected_features"] = features
                st.session_state["df"] = df

            # Prediction section: only show if model is trained
            if st.session_state.get("trained_model") is not None:
                st.subheader("Make a Prediction")
                input_data = {}
                df_for_input = st.session_state["df"]
                numeric_features = st.session_state["numeric_features"]
                categorical_features = st.session_state["categorical_features"]
                selected_features = st.session_state["selected_features"]

                # Input widgets for each feature
                for feature in selected_features:
                    if feature in numeric_features:
                        val = st.number_input(f"Input value for {feature}", value=float(df_for_input[feature].mean()))
                    else:
                        val = st.selectbox(f"Input value for {feature}", options=df_for_input[feature].unique())
                    input_data[feature] = val

                # Initialize prediction result in session state
                if "prediction_result" not in st.session_state:
                    st.session_state["prediction_result"] = None

                # Button to make prediction
                if st.button("Predict", key="predict_btn"):
                    input_df = pd.DataFrame([input_data])
                    pred = st.session_state["trained_model"].predict(input_df)[0]
                    st.session_state["prediction_result"] = f"Predicted value: {pred:.4f}"

                # Show prediction result
                if st.session_state.get("prediction_result"):
                    st.write(st.session_state["prediction_result"])
```

**Explanation:**
**Code Snippets & Explanation:**

- **CSV Upload & Data Preview:**
  ```python
  uploaded_file = st.file_uploader("Upload CSV file", type=["csv"])
  if uploaded_file:
      df = pd.read_csv(uploaded_file)
      st.write("Data Preview:", df.head())
  ```
  Lets users upload a CSV and shows the first few rows.

- **Data Summary & Correlation Matrix:**
  ```python
  st.subheader("Data Summary")
  st.write("Number of missing values:")
  st.write(df.isnull().sum())
  st.write("Median values:")
  st.write(df.select_dtypes(include=['number']).median())
  st.write("Mean values:")
  st.write(df.select_dtypes(include=['number']).mean())

  st.subheader("Correlation Matrix")
  corr_matrix = df.corr(numeric_only=True)
  st.write(corr_matrix)
  fig = plt.figure(figsize=(20, 16))
  sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", square=True, cbar_kws={"shrink": .8})
  st.pyplot(fig)
  ```
  Shows missing values, medians, means, and a heatmap of correlations.

- **Feature & Target Selection:**
  ```python
  target = st.selectbox("Select label (Y) column", df.columns)
  features = st.multiselect("Select feature columns (X)", [col for col in df.columns if col != target])
  ```
  Lets users pick the target and features for regression.

- **Model Training Pipeline:**
  ```python
  if st.button("Start Training", key="start_training_btn"):
      clean_df = df[features + [target]].dropna()
      X = clean_df[features]
      y = clean_df[target]

      numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
      categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()

      preprocessor = ColumnTransformer(
          transformers=[
              ('num', StandardScaler(), numeric_features),
              ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
          ]
      )

      model = Pipeline(steps=[
          ('preprocessor', preprocessor),
          ('regressor', XGBRegressor(max_depth=3, n_estimators=100, random_state=42))
      ])

      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
      model.fit(X_train, y_train)
      score = model.score(X_test, y_test)
      y_pred = model.predict(X_test)
      mae = mean_absolute_error(y_test, y_pred)
      mse = mean_squared_error(y_test, y_pred)

      st.write(f"Model R^2 score on test set: {score:.4f}")
      st.write(f"Mean Absolute Error (MAE): {mae:.4f}")
      st.write(f"Mean Squared Error (MSE): {mse:.4f}")
  ```
  Trains an XGBoost regression model with preprocessing and displays metrics.

- **Prediction Input & Output:**
  ```python
  if st.session_state.get("trained_model") is not None:
      st.subheader("Make a Prediction")
      input_data = {}
      for feature in selected_features:
          if feature in numeric_features:
              val = st.number_input(f"Input value for {feature}", value=float(df_for_input[feature].mean()))
          else:
              val = st.selectbox(f"Input value for {feature}", options=df_for_input[feature].unique())
          input_data[feature] = val

      if st.button("Predict", key="predict_btn"):
          input_df = pd.DataFrame([input_data])
          pred = st.session_state["trained_model"].predict(input_df)[0]
          st.session_state["prediction_result"] = f"Predicted value: {pred:.4f}"

      if st.session_state.get("prediction_result"):
          st.write(st.session_state["prediction_result"])
  ```
  Lets users input feature values and shows the predicted result.

These snippets illustrate the main logic for uploading data, training a custom XGBoost regression model, and making predictions interactively in Streamlit.

## 3. How to Run the App

1. **Start the Streamlit app:**
   ```bash
   streamlit run main.py
   ```

2. **Navigate using the sidebar:**
   - "House Price Prediction": Use the pre-trained model for predictions.
   - "Custom Linear Regression": Upload your own data, train, and predict.
   - "Custom XGBoost": Same as above, but with XGBoost.

3. **Train Models:**
   - For custom models, upload a CSV, select features/target, and click "Start Training".
   - After training, use the input widgets to make predictions.

4. **About Tab:**
   - Provides a brief description of the app.

---

## 4. Key Dependencies

- `streamlit`
- `pandas`
- `scikit-learn`
- `xgboost`
- `matplotlib`
- `seaborn`
- `pydantic` (for input validation)

---

## 5. Customization

- To add new tabs, update `configs/config.py` and `main.py`.
- To change model logic, edit the respective files in `views/`.
- For new features, follow the modular structure used in the current codebase.

---

## 6. Troubleshooting

- If you see "Trained model file not found", run the notebook to generate `linear_regression_model.pkl`.
- Ensure all dependencies are installed.
- For issues with CSV uploads, check your data format and column names.

---

# End of Guide
